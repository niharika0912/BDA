{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rv3lkph_KF30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252f4752-706a-47f2-8714-9977d2be96f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4525 - loss: 1.7411 - val_accuracy: 0.9141 - val_loss: 0.2917\n",
            "Epoch 2/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8473 - loss: 0.5206 - val_accuracy: 0.9434 - val_loss: 0.1847\n",
            "Epoch 3/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9025 - loss: 0.3444 - val_accuracy: 0.9566 - val_loss: 0.1446\n",
            "Epoch 4/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9261 - loss: 0.2662 - val_accuracy: 0.9647 - val_loss: 0.1185\n",
            "Epoch 5/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9388 - loss: 0.2158 - val_accuracy: 0.9693 - val_loss: 0.1047\n",
            "Epoch 6/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9484 - loss: 0.1840 - val_accuracy: 0.9732 - val_loss: 0.0958\n",
            "Epoch 7/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9577 - loss: 0.1519 - val_accuracy: 0.9742 - val_loss: 0.0918\n",
            "Epoch 8/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9627 - loss: 0.1289 - val_accuracy: 0.9752 - val_loss: 0.0885\n",
            "Epoch 9/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9670 - loss: 0.1149 - val_accuracy: 0.9767 - val_loss: 0.0852\n",
            "Epoch 10/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.1092 - val_accuracy: 0.9783 - val_loss: 0.0815\n",
            "Epoch 11/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0926 - val_accuracy: 0.9791 - val_loss: 0.0794\n",
            "Epoch 12/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0889 - val_accuracy: 0.9787 - val_loss: 0.0785\n",
            "Epoch 13/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9770 - loss: 0.0762 - val_accuracy: 0.9797 - val_loss: 0.0789\n",
            "Epoch 14/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9789 - loss: 0.0696 - val_accuracy: 0.9800 - val_loss: 0.0804\n",
            "Epoch 15/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0640 - val_accuracy: 0.9798 - val_loss: 0.0790\n",
            "Epoch 16/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9812 - loss: 0.0607 - val_accuracy: 0.9808 - val_loss: 0.0760\n",
            "Epoch 17/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9840 - loss: 0.0569 - val_accuracy: 0.9807 - val_loss: 0.0788\n",
            "Epoch 18/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0513 - val_accuracy: 0.9807 - val_loss: 0.0759\n",
            "Epoch 19/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9843 - loss: 0.0527 - val_accuracy: 0.9808 - val_loss: 0.0765\n",
            "Epoch 20/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9868 - loss: 0.0450 - val_accuracy: 0.9809 - val_loss: 0.0818\n",
            "Epoch 21/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9874 - loss: 0.0413 - val_accuracy: 0.9815 - val_loss: 0.0775\n",
            "Epoch 22/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9870 - loss: 0.0419 - val_accuracy: 0.9819 - val_loss: 0.0776\n",
            "Epoch 23/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9877 - loss: 0.0399 - val_accuracy: 0.9817 - val_loss: 0.0835\n",
            "Epoch 24/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9882 - loss: 0.0385 - val_accuracy: 0.9819 - val_loss: 0.0795\n",
            "Epoch 25/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0354 - val_accuracy: 0.9827 - val_loss: 0.0785\n",
            "Epoch 26/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.0298 - val_accuracy: 0.9822 - val_loss: 0.0767\n",
            "Epoch 27/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0288 - val_accuracy: 0.9822 - val_loss: 0.0783\n",
            "Epoch 28/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0290 - val_accuracy: 0.9819 - val_loss: 0.0808\n",
            "Epoch 29/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9900 - loss: 0.0328 - val_accuracy: 0.9819 - val_loss: 0.0832\n",
            "Epoch 30/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9918 - loss: 0.0276 - val_accuracy: 0.9832 - val_loss: 0.0802\n",
            "Epoch 31/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9910 - loss: 0.0287 - val_accuracy: 0.9826 - val_loss: 0.0765\n",
            "Epoch 32/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0239 - val_accuracy: 0.9837 - val_loss: 0.0751\n",
            "Epoch 33/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9921 - loss: 0.0257 - val_accuracy: 0.9840 - val_loss: 0.0766\n",
            "Epoch 34/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9920 - loss: 0.0242 - val_accuracy: 0.9831 - val_loss: 0.0766\n",
            "Epoch 35/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9916 - loss: 0.0264 - val_accuracy: 0.9833 - val_loss: 0.0780\n",
            "Epoch 36/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9934 - loss: 0.0218 - val_accuracy: 0.9833 - val_loss: 0.0787\n",
            "Epoch 37/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9936 - loss: 0.0226 - val_accuracy: 0.9835 - val_loss: 0.0794\n",
            "Epoch 38/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0245 - val_accuracy: 0.9834 - val_loss: 0.0781\n",
            "Epoch 39/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9935 - loss: 0.0200 - val_accuracy: 0.9843 - val_loss: 0.0758\n",
            "Epoch 40/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0213 - val_accuracy: 0.9841 - val_loss: 0.0787\n",
            "Epoch 41/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0231 - val_accuracy: 0.9834 - val_loss: 0.0816\n",
            "Epoch 42/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9940 - loss: 0.0196 - val_accuracy: 0.9832 - val_loss: 0.0820\n",
            "Epoch 43/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9949 - loss: 0.0163 - val_accuracy: 0.9821 - val_loss: 0.0842\n",
            "Epoch 44/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0175 - val_accuracy: 0.9836 - val_loss: 0.0810\n",
            "Epoch 45/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9939 - loss: 0.0199 - val_accuracy: 0.9836 - val_loss: 0.0760\n",
            "Epoch 46/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9949 - loss: 0.0171 - val_accuracy: 0.9837 - val_loss: 0.0792\n",
            "Epoch 47/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9947 - loss: 0.0181 - val_accuracy: 0.9843 - val_loss: 0.0782\n",
            "Epoch 48/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9941 - loss: 0.0186 - val_accuracy: 0.9836 - val_loss: 0.0849\n",
            "Epoch 49/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0149 - val_accuracy: 0.9850 - val_loss: 0.0786\n",
            "Epoch 50/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0160 - val_accuracy: 0.9845 - val_loss: 0.0766\n",
            "Epoch 51/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9948 - loss: 0.0176 - val_accuracy: 0.9843 - val_loss: 0.0798\n",
            "Epoch 52/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9949 - loss: 0.0172 - val_accuracy: 0.9847 - val_loss: 0.0750\n",
            "Epoch 53/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9946 - loss: 0.0165 - val_accuracy: 0.9846 - val_loss: 0.0747\n",
            "Epoch 54/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0161 - val_accuracy: 0.9833 - val_loss: 0.0815\n",
            "Epoch 55/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9958 - loss: 0.0146 - val_accuracy: 0.9837 - val_loss: 0.0817\n",
            "Epoch 56/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0155 - val_accuracy: 0.9838 - val_loss: 0.0825\n",
            "Epoch 57/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9943 - loss: 0.0192 - val_accuracy: 0.9833 - val_loss: 0.0799\n",
            "Epoch 58/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0149 - val_accuracy: 0.9831 - val_loss: 0.0829\n",
            "Epoch 59/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0157 - val_accuracy: 0.9849 - val_loss: 0.0817\n",
            "Epoch 60/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0136 - val_accuracy: 0.9858 - val_loss: 0.0773\n",
            "Epoch 61/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0116 - val_accuracy: 0.9847 - val_loss: 0.0761\n",
            "Epoch 62/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9958 - loss: 0.0146 - val_accuracy: 0.9837 - val_loss: 0.0796\n",
            "Epoch 63/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0122 - val_accuracy: 0.9852 - val_loss: 0.0787\n",
            "Epoch 64/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9955 - loss: 0.0147 - val_accuracy: 0.9833 - val_loss: 0.0838\n",
            "Epoch 65/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9949 - loss: 0.0167 - val_accuracy: 0.9849 - val_loss: 0.0778\n",
            "Epoch 66/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9959 - loss: 0.0125 - val_accuracy: 0.9851 - val_loss: 0.0755\n",
            "Epoch 67/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9960 - loss: 0.0125 - val_accuracy: 0.9844 - val_loss: 0.0836\n",
            "Epoch 68/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0114 - val_accuracy: 0.9841 - val_loss: 0.0796\n",
            "Epoch 69/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0100 - val_accuracy: 0.9836 - val_loss: 0.0822\n",
            "Epoch 70/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9960 - loss: 0.0128 - val_accuracy: 0.9844 - val_loss: 0.0842\n",
            "Epoch 71/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0126 - val_accuracy: 0.9847 - val_loss: 0.0867\n",
            "Epoch 72/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0110 - val_accuracy: 0.9854 - val_loss: 0.0786\n",
            "Epoch 73/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9964 - loss: 0.0115 - val_accuracy: 0.9843 - val_loss: 0.0802\n",
            "Epoch 74/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0109 - val_accuracy: 0.9847 - val_loss: 0.0798\n",
            "Epoch 75/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0116 - val_accuracy: 0.9838 - val_loss: 0.0819\n",
            "Epoch 76/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0121 - val_accuracy: 0.9845 - val_loss: 0.0826\n",
            "Epoch 77/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9959 - loss: 0.0132 - val_accuracy: 0.9849 - val_loss: 0.0808\n",
            "Epoch 78/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0105 - val_accuracy: 0.9850 - val_loss: 0.0791\n",
            "Epoch 79/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0097 - val_accuracy: 0.9841 - val_loss: 0.0813\n",
            "Epoch 80/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0097 - val_accuracy: 0.9846 - val_loss: 0.0821\n",
            "Epoch 81/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9967 - loss: 0.0123 - val_accuracy: 0.9844 - val_loss: 0.0814\n",
            "Epoch 82/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0089 - val_accuracy: 0.9841 - val_loss: 0.0825\n",
            "Epoch 83/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9974 - loss: 0.0088 - val_accuracy: 0.9844 - val_loss: 0.0855\n",
            "Epoch 84/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0101 - val_accuracy: 0.9847 - val_loss: 0.0820\n",
            "Epoch 85/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9975 - loss: 0.0082 - val_accuracy: 0.9843 - val_loss: 0.0844\n",
            "Epoch 86/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9970 - loss: 0.0108 - val_accuracy: 0.9841 - val_loss: 0.0874\n",
            "Epoch 87/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0110 - val_accuracy: 0.9848 - val_loss: 0.0839\n",
            "Epoch 88/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0085 - val_accuracy: 0.9847 - val_loss: 0.0840\n",
            "Epoch 89/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9974 - loss: 0.0089 - val_accuracy: 0.9844 - val_loss: 0.0863\n",
            "Epoch 90/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0102 - val_accuracy: 0.9854 - val_loss: 0.0795\n",
            "Epoch 91/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0113 - val_accuracy: 0.9852 - val_loss: 0.0780\n",
            "Epoch 92/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0092 - val_accuracy: 0.9853 - val_loss: 0.0834\n",
            "Epoch 93/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9972 - loss: 0.0090 - val_accuracy: 0.9847 - val_loss: 0.0868\n",
            "Epoch 94/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0104 - val_accuracy: 0.9860 - val_loss: 0.0778\n",
            "Epoch 95/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9974 - loss: 0.0084 - val_accuracy: 0.9858 - val_loss: 0.0784\n",
            "Epoch 96/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9968 - loss: 0.0093 - val_accuracy: 0.9857 - val_loss: 0.0740\n",
            "Epoch 97/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0093 - val_accuracy: 0.9852 - val_loss: 0.0807\n",
            "Epoch 98/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0091 - val_accuracy: 0.9859 - val_loss: 0.0781\n",
            "Epoch 99/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0085 - val_accuracy: 0.9862 - val_loss: 0.0780\n",
            "Epoch 100/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0083 - val_accuracy: 0.9853 - val_loss: 0.0815\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.0889\n",
            "Test accuracy: 0.9865000247955322\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data: normalize images and one-hot encode labels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input (28x28 images) into a vector of size 784\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Add 5 hidden layers with increased neurons and Batch Normalization\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Add the output layer with 10 neurons (one for each class) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model using the 'adam' optimizer with a lower learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with increased epochs\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    }
  ]
}